{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is Cross-Validation Important?\n",
    "In machine learning, our goal is to build a model that performs well on new, unseen data, not just on the training data. Cross-validation helps in:\n",
    "\n",
    "Generalization: Ensuring that the model performs well on different datasets.\n",
    "\n",
    "Reducing Overfitting: Preventing the model from being too specific to the training data.\n",
    "\n",
    "Low Variance: Achieving consistent performance across different datasets.\n",
    "\n",
    "Basic Concept\n",
    "Typically, we split our dataset into training and testing sets. For instance, with a dataset of 1000 records, we might use 80% (800 records) for training and 20% (200 records) for testing. However, a single train-test split might not be sufficient to evaluate the model reliably. This is where cross-validation comes in.\n",
    "\n",
    "Types of Cross-Validation Techniques\n",
    "1. Leave-One-Out Cross-Validation (LOOCV)\n",
    "Description: In LOOCV, each data point is used once as a validation set while the remaining points form the training set. This process is repeated for all data points.\n",
    "\n",
    "Process:\n",
    "For a dataset with n points, train on n−1 points and test on the remaining one.\n",
    "Repeat for all data points and average the results.\n",
    "Example: With 5 records, each record will be the validation set in 5 different iterations.\n",
    "\n",
    "2. Hold-Out Cross-Validation\n",
    "Description: The dataset is split into two parts: a training set and a validation set. Typically, 80% of the data is used for training and 20% for validation.\n",
    "\n",
    "Process:\n",
    "Train the model on the training set and validate it on the validation set.\n",
    "This method is simple but may not provide a reliable performance estimate.\n",
    "Example: With 800 training records, split into 80% training and 20% validation repeatedly.\n",
    "\n",
    "3. K-Fold Cross-Validation\n",
    "Description: The dataset is divided into 'k' subsets (folds). The model is trained on k−1 folds and tested on the remaining fold. This process is repeated 'k' times.\n",
    "\n",
    "Process:\n",
    "For example, with k=10, the dataset is split into 10 folds, each containing 100 records.\n",
    "\n",
    "In each iteration, one fold is used for validation, and the remaining 9 folds are used for training.\n",
    "\n",
    "Average the performance metrics across all folds.\n",
    "\n",
    "4. Stratified K-Fold Cross-Validation\n",
    "Description: Similar to K-Fold but ensures each fold has the same proportion of class labels as the original dataset.\n",
    "\n",
    "Use Case: Particularly useful for imbalanced datasets.\n",
    "\n",
    "Process: Ensures class proportions are maintained in each fold, balancing 0 and 1 classes in a binary classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of Applying K-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-Fold Cross-Validation Scores: [1.         0.96666667 0.93333333 0.93333333 0.96666667]\n",
      "Mean Accuracy: 0.9600000000000002\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import cross_val_score, KFold, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Create a RandomForest classifier\n",
    "clf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# K-Fold Cross-Validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scores = cross_val_score(clf, X, y, cv=kf, scoring='accuracy')\n",
    "\n",
    "print(\"K-Fold Cross-Validation Scores:\", scores)\n",
    "print(\"Mean Accuracy:\", np.mean(scores))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
